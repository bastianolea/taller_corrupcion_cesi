
```{r}
library(rvest)
library(dplyr)
```

# Scraping de enlaces a noticias

```{r}
sitio <- "https://www.biobiochile.cl/lista/categorias/nacional" |>
  read_html()

titulos <- sitio |>
  html_elements(".article-text-container")

titulos |> html_text2()

enlaces <- titulos  |> 
  html_element("a") |>
  html_attr("href")

enlaces
```

Scraping de una noticia
```{r}
enlace <- enlaces[1]

pagina <- enlace |> 
  read_html()
```

Título
```{r}
titulo <- pagina |> 
  html_element(".post-title") |> 
  html_text2()
```

Fecha
```{r}
pagina |> 
  html_elements(".post-date") |> 
  html_text2()

fecha <- enlace |> 
  stringr::str_extract("\\d{4}/\\d{2}/\\d{2}") |> 
  lubridate::as_date()
```

Cuerpo
```{r}
cuerpo <- pagina |> 
  html_elements(".container-redes-contenido") |> 
  html_text2() |> 
  paste(collapse = "\n")
```

Unir
```{r}
noticia <- tibble(titulo,
       fecha,
       cuerpo)
```


## Scraping de múltiples noticias

```{r}
library(purrr)

enlaces2 <- enlaces[!is.na(enlaces)]

noticias <- map(enlaces2, \(enlace) {
  
  message(enlace)
  
  pagina <- enlace |> 
    read_html()
  
  # título
  titulo <- pagina |> 
    html_element(".post-title") |> 
    html_text2()
  
  # fecha
  pagina |> 
    html_elements(".post-date") |> 
    html_text2()
  
  fecha <- enlace |> 
    stringr::str_extract("\\d{4}/\\d{2}/\\d{2}") |> 
    lubridate::as_date()
  
  # cuerpo
  cuerpo <- pagina |> 
    html_elements(".container-redes-contenido") |> 
    html_text2() |> 
    paste(collapse = "\n")
  
  
  noticia <- tibble(titulo,
                    fecha,
                    cuerpo,
                    enlace)
})

noticias2 <- noticias |> 
  list_rbind() |> 
  filter(!is.na(titulo))

noticias2
```

guardar

```{r}
write.csv2(noticias2, "datos/noticias.csv")
```



## Procesar texto

```{r}
library(readr)
```

Cargar resultados del paso anterior
```{r}
noticias <- read_csv2("datos/noticias.csv")


noticias

library(tidytext)
```


Tokenizar

```{r}
palabras <- noticias |> 
  unnest_tokens(output = "palabra",
                input = cuerpo) 

palabras
```

Tokenizar removiendo palabras vacías

```{r}
palabras <- noticias |> 
  unnest_tokens(output = "palabra",
                input = cuerpo) |> 
  filter(!palabra %in% stopwords::stopwords("es"))

palabras
```

### Limpiar texto

```{r}
eliminar <- c("así", "lee", "julio", "si", "tras", "dos", "martes", "content", "https", "lunes", "media.biobiochile.cl", "uploads", "wp", "bío")


library(stringr)

noticias_2 <- noticias |> 
  mutate(cuerpo_limpio = str_to_lower(cuerpo)) |> 
  # es mejor convertir a espacios que eliminar, porque así se separan de la anterior/siguiente palabra
  mutate(cuerpo_limpio = str_remove_all(cuerpo_limpio, "[[:punct:]]"),
         cuerpo_limpio = str_remove_all(cuerpo_limpio, "[[:digit:]]")) |> 
  mutate(cuerpo_limpio = str_trim(cuerpo_limpio))
  
```


Tokenizar sobre texto limpio, removiendo palabras vacías, otras palabras, y símbolos

```{r}
palabras <- noticias_2 |> 
  unnest_tokens(output = "palabra",
                input = cuerpo_limpio) |> 
  # eliminar palabras
  filter(!palabra %in% stopwords::stopwords("es"),
         !palabra %in% eliminar)
```

```{r}
palabras |> 
  count(palabra, sort = TRUE) |> 
  print(n=30)
```

```{r}
palabras |> 
  filter(palabra  == "jara") |> 
  distinct(titulo)
```

```{r}
palabras |> 
  filter(palabra  == "kast") |> 
  distinct(titulo)
```

```{r}
palabras |> 
  filter(palabra  == "fiscal") |> 
  distinct(titulo)
```


