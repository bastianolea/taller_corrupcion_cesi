library(rvest)
library(dplyr)

# scraping de enlaces a noticias
sitio <- "https://www.biobiochile.cl/lista/categorias/nacional" |>
  read_html()

titulos <- sitio |>
  html_elements(".article-text-container")

titulos |> html_text2()

enlaces <- titulos  |> 
  html_element("a") |>
  html_attr("href")

enlaces

# scraping de una noticia
enlace <- enlaces[1]

pagina <- enlace |> 
  read_html()

# título
titulo <- pagina |> 
  html_element(".post-title") |> 
  html_text2()

# fecha
pagina |> 
  html_elements(".post-date") |> 
  html_text2()

fecha <- enlace |> 
  stringr::str_extract("\\d{4}/\\d{2}/\\d{2}") |> 
  lubridate::as_date()

# cuerpo
cuerpo <- pagina |> 
  html_elements(".container-redes-contenido") |> 
  html_text2() |> 
  paste(collapse = "\n")


noticia <- tibble(titulo,
       fecha,
       cuerpo)

enlaces2 <- enlaces[!is.na(enlaces)]


# scraping de múltiples noticias
library(purrr)

noticias <- map(enlaces2, \(enlace) {
  
  message(enlace)
  
  pagina <- enlace |> 
    read_html()
  
  # título
  titulo <- pagina |> 
    html_element(".post-title") |> 
    html_text2()
  
  # fecha
  pagina |> 
    html_elements(".post-date") |> 
    html_text2()
  
  fecha <- enlace |> 
    stringr::str_extract("\\d{4}/\\d{2}/\\d{2}") |> 
    lubridate::as_date()
  
  # cuerpo
  cuerpo <- pagina |> 
    html_elements(".container-redes-contenido") |> 
    html_text2() |> 
    paste(collapse = "\n")
  
  
  noticia <- tibble(titulo,
                    fecha,
                    cuerpo,
                    enlace)
})

noticias2 <- noticias |> 
  list_rbind() |> 
  filter(!is.na(titulo))

noticias2

# guardar
write.csv2(noticias2, "datos/noticias.csv")



# procesar ----
library(readr)

# cargar resultados del paso anterior
noticias <- read_csv2("datos/noticias.csv")


noticias

library(tidytext)

# tokenizar
palabras <- noticias |> 
  unnest_tokens(output = "palabra",
                input = cuerpo) 

palabras

# tokenizar removiendo palabras vacías
palabras <- noticias |> 
  unnest_tokens(output = "palabra",
                input = cuerpo) |> 
  filter(!palabra %in% stopwords::stopwords("es"))

palabras

# definir más palabras vacías
eliminar <- c("así", "lee", "julio", "si", "tras", "dos", "martes", "content", "https", "lunes", "media.biobiochile.cl", "uploads", "wp", "bío")


library(stringr)

noticias_2 <- noticias |> 
  mutate(cuerpo_limpio = str_to_lower(cuerpo)) |> 
  # es mejor convertir a espacios que eliminar, porque así se separan de la anterior/siguiente palabra
  mutate(cuerpo_limpio = str_remove_all(cuerpo_limpio, "[[:punct:]]"),
         cuerpo_limpio = str_remove_all(cuerpo_limpio, "[[:digit:]]")) |> 
  mutate(cuerpo_limpio = str_trim(cuerpo_limpio))
  
# tokenizar sobre texto limpio, removiendo palabras vacías, otras palabras, y símbolos
palabras <- noticias_2 |> 
  unnest_tokens(output = "palabra",
                input = cuerpo_limpio) |> 
  # eliminar palabras
  filter(!palabra %in% stopwords::stopwords("es"),
         !palabra %in% eliminar)

palabras |> 
  count(palabra, sort = TRUE) |> 
  print(n=30)

palabras |> 
  filter(palabra  == "jara") |> 
  distinct(titulo)

palabras |> 
  filter(palabra  == "kast") |> 
  distinct(titulo)

palabras |> 
  filter(palabra  == "fiscal") |> 
  distinct(titulo)

